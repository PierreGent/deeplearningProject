{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labels.csv', '.DS_Store', 'test', 'test.zip', 'train', 'train.zip', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from os import listdir\n",
    "from os.path import basename,join,exists\n",
    "import os\n",
    "print(listdir(\"./dog-breed-identification\"))\n",
    "import threading\n",
    "from queue import Queue\n",
    "from math import floor\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "['affenpinscher', 'afghan_hound', 'african_hunting_dog', 'airedale', 'american_staffordshire_terrier', 'appenzeller', 'australian_terrier', 'basenji', 'basset', 'beagle', 'bedlington_terrier', 'bernese_mountain_dog', 'black-and-tan_coonhound', 'blenheim_spaniel', 'bloodhound', 'bluetick', 'border_collie', 'border_terrier', 'borzoi', 'boston_bull', 'bouvier_des_flandres', 'boxer', 'brabancon_griffon', 'briard', 'brittany_spaniel', 'bull_mastiff', 'cairn', 'cardigan', 'chesapeake_bay_retriever', 'chihuahua', 'chow', 'clumber', 'cocker_spaniel', 'collie', 'curly-coated_retriever', 'dandie_dinmont', 'dhole', 'dingo', 'doberman', 'english_foxhound', 'english_setter', 'english_springer', 'entlebucher', 'eskimo_dog', 'flat-coated_retriever', 'french_bulldog', 'german_shepherd', 'german_short-haired_pointer', 'giant_schnauzer', 'golden_retriever', 'gordon_setter', 'great_dane', 'great_pyrenees', 'greater_swiss_mountain_dog', 'groenendael', 'ibizan_hound', 'irish_setter', 'irish_terrier', 'irish_water_spaniel', 'irish_wolfhound', 'italian_greyhound', 'japanese_spaniel', 'keeshond', 'kelpie', 'kerry_blue_terrier', 'komondor', 'kuvasz', 'labrador_retriever', 'lakeland_terrier', 'leonberg', 'lhasa', 'malamute', 'malinois', 'maltese_dog', 'mexican_hairless', 'miniature_pinscher', 'miniature_poodle', 'miniature_schnauzer', 'newfoundland', 'norfolk_terrier', 'norwegian_elkhound', 'norwich_terrier', 'old_english_sheepdog', 'otterhound', 'papillon', 'pekinese', 'pembroke', 'pomeranian', 'pug', 'redbone', 'rhodesian_ridgeback', 'rottweiler', 'saint_bernard', 'saluki', 'samoyed', 'schipperke', 'scotch_terrier', 'scottish_deerhound', 'sealyham_terrier', 'shetland_sheepdog', 'shih-tzu', 'siberian_husky', 'silky_terrier', 'soft-coated_wheaten_terrier', 'staffordshire_bullterrier', 'standard_poodle', 'standard_schnauzer', 'sussex_spaniel', 'tibetan_mastiff', 'tibetan_terrier', 'toy_poodle', 'toy_terrier', 'vizsla', 'walker_hound', 'weimaraner', 'welsh_springer_spaniel', 'west_highland_white_terrier', 'whippet', 'wire-haired_fox_terrier', 'yorkshire_terrier']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./dog-breed-identification/sample_submission.csv')\n",
    "train_dir_path = \"./dog-breed-identification/train\"\n",
    "test_dir_path = \"./dog-breed-identification/test\"\n",
    "#pickled_dir_path  = \"../output/pickled_Data\"\n",
    "labels_df = pd.read_csv('./dog-breed-identification/labels.csv')\n",
    "dog_breeds = list(df.columns[1:])\n",
    "print(len(dog_breeds))\n",
    "print(dog_breeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "6098db8e4d7ddc93df9fbc9d6497baef49f3e801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10222\n",
      "10357\n"
     ]
    }
   ],
   "source": [
    "train_img_fpaths = [ join(train_dir_path, f) for f in listdir(train_dir_path)]\n",
    "test_img_fpaths = [join(test_dir_path, f) for f in listdir(test_dir_path)]\n",
    "print(len(train_img_fpaths))\n",
    "print(len(test_img_fpaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "58bcd5d6a2118b5a3cea5c84ec37571fa21ba69b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dog_breed_from_id(dog_id):\n",
    "    #labels_df = pd.read_csv('../input/labels.csv')\n",
    "    return labels_df[labels_df['id'] ==dog_id]['breed'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "eb76349dd3eeefe06360740a6d6ec0b5388641e6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "6c16cccefbe3ffa886bf6f83b465bde1635651f5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables \n",
    "IMG_HEIGHT = 150\n",
    "IMG_WIDTH = 150\n",
    "IMG_CHANNELS = 3\n",
    "BATCH_SIZE = 500\n",
    "lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "1f055c5c6c2dc2c007c0ac498550d512c4244ecc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_to_array(img_path):   \n",
    "    img_array = cv.imread(img_path)\n",
    "    img_array = cv.resize(img_array, (IMG_HEIGHT, IMG_WIDTH))\n",
    "    img_array = img_array.reshape(-1,IMG_HEIGHT,IMG_WIDTH, IMG_CHANNELS)\n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "fc7b7e685bbe0b103a4a3eaeedaec962fe249365",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize queue which is threadsafe \n",
    "def initialize_queue():\n",
    "    queue =Queue()\n",
    "    return queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "a42b29fd5d8df0c903679dc636e0d866bd01cf24",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get list of image ids from names of test images\n",
    "def get_test_image_ids():\n",
    "    return [basename(fpath).split('.')[0] for fpath in test_img_fpaths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "64f447aca7bea745f4444f81e34ef139fb0c2b74",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converts image files to numpy array and based on train/test, return train array and labels,\n",
    "def get_data(is_train):\n",
    "    # 1 batch per thread and last thread with remaining images\n",
    "    img_fpaths = train_img_fpaths if is_train else test_img_fpaths\n",
    "    num_threads = floor(len(img_fpaths)/BATCH_SIZE)\n",
    "    print(\"num of threads:\", num_threads + 1)\n",
    "    img_array = None\n",
    "    queue = initialize_queue()\n",
    "    results = []          # results from multiple threads\n",
    "    print(\"getting training data....\") if is_train else print(\"getting testing data....\")\n",
    "    \n",
    "    # load queue with data for each task\n",
    "    for batch_index in range(num_threads + 1):\n",
    "        if batch_index == num_threads:\n",
    "            file_batch = img_fpaths[(batch_index*BATCH_SIZE):]\n",
    "        else:\n",
    "            file_batch = img_fpaths[(batch_index*BATCH_SIZE) : (batch_index + 1)*BATCH_SIZE]\n",
    "        queue.put(file_batch)\n",
    "    \n",
    "    # iterate over loop to create threads\n",
    "    for thread_index in range(num_threads+1):\n",
    "        thread = threading.Thread(target = get_train_data_parallely, args=(queue, results)) if is_train else threading.Thread(target =get_testing_data_parallely, args =(queue, results))    \n",
    "        thread.start()\n",
    "        print(\"{} started\".format(thread.name))\n",
    "       # worker_threads.append(thread)\n",
    "        \n",
    "    # when queue in empty\n",
    "    queue.join()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "b5738a2bea158ef8f8bdf880fa3c7670b0e38980",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert training data into list of tuples\n",
    "# this subroutine represents a task for child thread to collect training data\n",
    "def get_train_data_parallely(queue, results):\n",
    "    result = []\n",
    "    while not queue.empty():\n",
    "        fpaths = queue.get()\n",
    "        for f_path in fpaths:\n",
    "            img_array = img_to_array(f_path)\n",
    "            # train_img_array = img_array if train_img_array is None else np.vstack((train_img_array, img_array))\n",
    "            img_name = basename(f_path)\n",
    "            img_id = img_name.split('.')[0]\n",
    "            dog_breed = dog_breed_from_id(img_id)\n",
    "            #train_labels.append(dog_breed)\n",
    "            results.append((img_array, dog_breed))\n",
    "            \n",
    "    # append arr,labels for current task to results\n",
    "    print(\"{} finished\".format(threading.currentThread().getName()))\n",
    "    # signal for task has been done\n",
    "    queue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "cafe3280ff1a83135b0b1922c1374cea62b0051e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# worker job for converting test imgs to array\n",
    "def get_testing_data_parallely(queue, results):\n",
    "    while not queue.empty():\n",
    "        file_batch = queue.get()\n",
    "        for f_path in file_batch:        \n",
    "            img_name = basename(f_path)\n",
    "            img_id = img_name.split('.')[0]\n",
    "            results.append((img_id, img_to_array(f_path)))\n",
    "    print(\"{} finished\".format(threading.currentThread().getName()))\n",
    "    queue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "32ee84979d6990fe6139247310555303555bfb77",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# method for getting training data\n",
    "def get_training_data():\n",
    "    train_results = get_data(is_train = True)\n",
    "    train_labels = []\n",
    "    img_arrays= []\n",
    "    for u_index in range(len(train_results)):\n",
    "        img_arr, identified_breed = train_results[u_index]\n",
    "        img_arrays.append(img_arr)\n",
    "        train_labels.append(identified_breed)\n",
    "    train_arr = np.array(img_arrays).reshape(-1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "    train_arr = train_arr/255\n",
    "    train_labels = one_hot_encode_labels(train_labels)\n",
    "    return train_arr,train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "f1e663f373aa1ec0f23cba45ef974e68def66fe8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method for getting testing arr\n",
    "def get_testing_data():\n",
    "    results = get_data(is_train = False)\n",
    "    test_img_ids = []\n",
    "    test_img_list = []\n",
    "    for test_result in results:\n",
    "        img_id, img_arr = test_result\n",
    "        test_img_list.append(img_arr)\n",
    "        test_img_ids.append(img_id)\n",
    "    test_img_arr = np.array(test_img_list).reshape(-1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "    test_img_arr = test_img_arr/255\n",
    "    return test_img_arr, test_img_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "22fa7ae953f78d33b2b4798bec2d9d4b00ab79b7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_obj_to_disk(fname, obj):\n",
    "    print(\"saving \"+ fname +\" to filesystem\")\n",
    "    if  exists(fname):\n",
    "        print(fname + \"already exists\") \n",
    "    with open(fname, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "b46cbdeb0037b857ec41a779366f0cd771545c0e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_obj_from_disk(fname):\n",
    "    if exists(fname):\n",
    "        print(\"loading \"+fname + \" from filesystem\")\n",
    "        obj = None\n",
    "        with open(fname, 'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "        return obj\n",
    "    else:\n",
    "        print(fname + \"doesnt not exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "94689997aba3c0a76a054c2644d60971e2f2ad88",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_test_data(load_train=False, load_test=False, one_hot_encode=False):\n",
    "    train_arr = None\n",
    "    train_labels = None\n",
    "    test_arr = None\n",
    "    \n",
    "    # check if training data and labels exists already as pickled file\n",
    "    if load_train:\n",
    "        if exists(\"train_data.pickle\") and exists(\"train_labels.pickle\"):\n",
    "            train_arr = load_obj_from_disk(\"train_data.pickle\")\n",
    "            train_labels = load_obj_from_disk(\"train_labels.pickle\")\n",
    "            if one_hot_encode:\n",
    "                train_labels = one_hot_encode_labels(train_labels)\n",
    "        else:\n",
    "            # create training_data and save it to filesystem\n",
    "            train_arr, train_labels = get_data(is_train= True)\n",
    "            if not exists(\"train_data.pickle\"):\n",
    "                save_obj_to_disk(\"train_data.pickle\", train_data)\n",
    "            if not exists(\"train_labels.pickle\"):\n",
    "                save_obj_to_disk(\"train_labels.pickle\", train_labels)\n",
    "        print(\" train array shape : {}, train array labels: {}\".format(train_data.shape,len(train_labels)))\n",
    "        \n",
    "    # check if testing data and labels exists already as pickled file\n",
    "    if load_test:\n",
    "        if exists(\"test_data.pickle\"):\n",
    "            test_arr = load_obj_from_disk(\"test_data.pickle\")\n",
    "        else:\n",
    "            # create test_data and save it to filesystem\n",
    "            test_arr = get_data(is_train= False)\n",
    "            save_obj_to_disk(\"test_data.pickle\", test_arr)\n",
    "        print(\" test array shape : {}\".format(test_arr.shape))\n",
    "    return train_arr, train_labels, test_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "acca546637834880440388d904555cdc2006b785",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method for onehot encoding labels of train_arr\n",
    "def one_hot_encode_labels(label_arr):\n",
    "    from sklearn.preprocessing import LabelEncoder ,OneHotEncoder\n",
    "    labelEncoder = LabelEncoder()\n",
    "    integer_encoded = labelEncoder.fit_transform(np.array(label_arr))\n",
    "    integer_encoded = integer_encoded.reshape(-1,1)\n",
    "    onehotEncoder = OneHotEncoder()\n",
    "    onehot_encoded_arr = onehotEncoder.fit_transform(integer_encoded).toarray()\n",
    "    return onehot_encoded_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "be449887b7d8ba9f9778c03bf4ad54e2bff5b1ff",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y =get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f01a2b6b2e086dd7b44dac15e99265b3f05ba66c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, valdn_x, train_y, valdn_y = train_test_split(x,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "59556f42676c9a12338e2aa74655f8b46ed15abe",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x, test_img_ids = get_testing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "9ea978f1c898090500eb4c36b276405592f13c30",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(valdn_x.shape)\n",
    "print(valdn_y.shape)\n",
    "print(test_x.shape)\n",
    "print(len(test_img_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "659fd8492fe15451e521c030cff7f0e5ec59961d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "5fd6ed73aeb66a653810f582574b2117542d15b6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# conv 1\n",
    "model.add(Conv2D(16, (3,3), input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)))       # input -N,150,150,3, output- N,148,148,16\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "# max pool 1\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2))                                   #input- N,148,148,16, output- N, 74,74,16\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# # conv 2\n",
    "model.add(Conv2D(32, (3,3)))                                                         #input- N,74,74,16 output - N, 72,72,16\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "# max pool 2\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2))                                 #input - N,72,72,16, output- N,36,36,16\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# conv 3\n",
    "model.add(Conv2D(48, (3,3)))                                                       #input - N,36,36,16, output- N,34,34,32\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.7))\n",
    "\n",
    "# max pool 3\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2))                                #input- N,34,34,32, output- N,17,17,32\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# # conv 4\n",
    "model.add(Conv2D(64, (3,3)))                                                     #input- N,17,17,32, output- N,15,15,32\n",
    "model.add(BatchNormalization(axis=3))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.7))\n",
    "# max pool 4\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2))                              #input- N,15,15,32, output- N,7,7,32\n",
    "\n",
    "# flatten\n",
    "model.add(Flatten())                                                            # output- 1568\n",
    "\n",
    "# fc layer 1\n",
    "model.add(Dense(1024, activation='relu'))                                  \n",
    "\n",
    "# fc layer 2\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "# fc layer 3\n",
    "model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# fc layer 4\n",
    "model.add(Dense(120, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "cfadef0004c484415660f40499bc2e7549bf5723",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "8bb3b45e9b7ec4bdcfa74ab003fd07bff3a0112c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile model for with softmax cross entropy and adam optimizer, set accuracy as parameter to evaluate\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "1575e46888536132b9548f6f7e448e547e209a3c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train model on training data\n",
    "model_hist = model.fit(train_x, train_y, batch_size=64, nb_epoch=100, verbose=1, validation_data=(valdn_x, valdn_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "e872bc8b28692ed635f1047ab6ba9a846e179017",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_x, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "caaa74d3c9226485c923e64d2fbb5d55e63c60a3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(predictions.shape)\n",
    "print(len(dog_breeds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "0c628627f62345fe1c8c7a4f2379db5f03fa2770",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "submission_res = pd.DataFrame(data= predictions, index =test_img_ids, columns= dog_breeds)\n",
    "submission_res.index.name = 'id'\n",
    "submission_res.to_csv('submission.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_uuid": "bb3fd827d6d129fd1a8b95ed51cfac23616e8798",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(model_hist.history['acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_uuid": "a30b60745c9d516d0fa635f769650e5ee6dc6d0b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(model_hist.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
